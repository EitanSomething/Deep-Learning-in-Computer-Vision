import os

import tensorflow_datasets as tfds
import numpy as np
from tqdm import tqdm

from process_data_api import _get_label_ids

PATH_TO_DATASET = "/home/ec2-user/Documents/datasets/foods-ss"

def _set_path():
    # Add the root of the venv to the path
    # This is necessary to import the selective_search module

    # Get the path to the venv
    venv_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    # Add the venv path to the path
    import sys
    sys.path.append(venv_path)

_set_path()

from generate_selective_search_examples import _crop_and_save_image, _get_ss_boxes_for_example, _convert_ss_boxes_to_coco, _convert_coco_bounds_to_PIL_bounds

def generate_background_examples(crops_per_image, source_dataset, label_ids_to_avoid, split):
    '''
    Generates background examples for the food dataset.
    The background examples are generated by taking crops from images in the Coco dataset.
    The crops are taken using the Selective Search algorithm.
    The crops are saved in the food dataset in the background directory.
    The crops are saved in the jpeg format.

    Parameters:
        crops_per_image: The number of crops to take from each image in the Coco dataset.
        source_dataset: The dataset to take the crops from.
        label_ids_to_avoid: The label ids to avoid when taking crops.
        split: The split of the food dataset to save the crops in.
    '''

    # Create directories in the food dataset for the background examples
    _create_background_directories(split)

    for example in tqdm(source_dataset):
        if _contains_label_in_subset(example, label_ids_to_avoid):
            continue

        ss_boxes = _get_n_boxes_with_selective_search(example, crops_per_image)
        # The boxes are stored in the Selective Search format.
        # Convert them to Coco format and then to PIL format
        pil_boxes = []
        img_shape = example["image"].shape
        for coco_box in _convert_ss_boxes_to_coco(ss_boxes, img_shape):
            pil_boxes.append(_convert_coco_bounds_to_PIL_bounds(coco_box, img_shape))

        for index, box in enumerate(pil_boxes):
            path_for_image = _get_path_for_cropped_example(split, example, index)
            _crop_and_save_image(example, box, path_for_image)

def _get_n_boxes_with_selective_search(example, n):
    '''
    Gets n boxes from an example using the Selective Search algorithm.

    Parameters:
        example: The example to get the boxes from.
        n: The number of boxes to get.
    
    Returns:
        A numpy array of n boxes.
    '''
    # Get the selective search boxes
    ss_boxes = _get_ss_boxes_for_example(example)

    # Select n boxes and return them
    boxes_per_skip = len(ss_boxes) // n

    selected_boxes = []
    for i in range(n):
        selected_boxes.append(ss_boxes[i * boxes_per_skip])
    
    return np.array(selected_boxes)

def _get_path_for_cropped_example(split, example, additional_token):
    '''
    Gets the path to save a cropped example to.

    Parameters:
        split: The split of the food dataset to save the example to.
        example: The example to save.
        additional_token: An additional token to add to the file name to make it unique.
    
    Returns:
        The path to save the example to.

    Notes:
        The path is in the format: PATH_TO_DATASET/split/background/id-additional_token.jpeg
    '''

    id = example["image/id"].numpy()
    file_name = str(id) + "-" + str(additional_token) + ".jpeg"
    path_to_file = os.path.join(PATH_TO_DATASET, split, "background", file_name)
    return path_to_file

def _create_background_directories(split):
    '''
    Creates the directories to save the background examples to.

    Parameters:
        split: The split of the food dataset to create the directories for.
    
    Notes:
        The directories are in the format: PATH_TO_DATASET/split/background
    '''

    split_path = os.path.join(PATH_TO_DATASET, split, "background")
    _ensure_directory_exists(split_path)

def _ensure_directory_exists(path):
        if not os.path.exists(path):
            os.makedirs(path)

def _contains_label_in_subset(example, label_subset):
    for label in example["objects"]["label"].numpy():
        if label in label_subset:
            return True
        
    return False

    

train_ds, info= tfds.load("coco/2017", split = "train", with_info = True, shuffle = True)
val_ds = tfds.load("coco/2017", split = "validation", shuffle = True)
all_labels = info.features["objects"]["label"].names

# We have generated 264,652 labeled examples in the train split of dataset so far.
# We need to generate five (background_factor) times that amount of background images.
# There are 102,032 images in Coco train without our labels. Those will be used to generate the background images.
# These lines determine how many crops should be taken from each image.
# The same logic is used to determine how many crops are taken from each image in Coco validation without our label
background_factor = 5
crops_per_train_example = int((264652 * background_factor) / 102032)
crops_per_validation_example = int((11941 * background_factor) / 4292)

example_labels = ["banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake"]
example_label_ids = _get_label_ids(example_labels, all_labels)

generate_background_examples(crops_per_train_example * 10, train_ds.take(10000), example_label_ids, split = "train")
generate_background_examples(crops_per_validation_example, val_ds, example_label_ids, split = "validation")